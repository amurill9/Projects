{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLC6bgG7vhK3"
   },
   "source": [
    "# CSE 146 Lab 1: Machine Learning Basics and Overfitting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ASSIGNED: Tuesday, October 6, 2020\n",
    "### DUE: Tuesday, October 13, 2020\n",
    "\n",
    "The purpose of this assignment is for you to gain experience with basic tools for training, evaluating and critiquing machine learning models.\n",
    "\n",
    "In this lab, you'll make use of Python 3, as well as standard Python libraries for data science pandas, scikit-learn, and matplotlib. If you're new to Python, we encourage you to examine the language documentation (https://docs.python.org/3/), which includes some simple tutorials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5CkZlLzvhK4"
   },
   "source": [
    "## Instructions \n",
    "This assignment can be done collaboratively, and please be sure to list the student(s) you worked with in the space provided below. Please reach out to each other if you have any questions or difficulties.\n",
    "\n",
    "Be sure to rename this homework notebook (in [YOUR NAME HERE] so that it includes your name. \n",
    "\n",
    "### List the student(s) you worked with on this assignment here:\n",
    "1. [person 1]\n",
    "2. [person 2]\n",
    "3. [etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZuJHKoevhK4"
   },
   "source": [
    "Before starting on this lab, you should make sure you have a solid understanding of machine learning basics. Please be sure to complete the listed modules in Google's [Introduction to Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3D7lMy3vhK5"
   },
   "source": [
    "# Problems\n",
    "## Part 1: Loading Data (10 points)\n",
    "\n",
    "First, we will import modules that will let us do lots quickly. We'll import some powerful tools for importing and working with data, especially `pandas`.\n",
    "\n",
    "Pandas has been in development since 2008, largely through the efforts of one developer. For more on the history see [here](https://en.wikipedia.org/wiki/Pandas_(software))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5Ie4zRlvhK6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyShCQZCvhK-"
   },
   "source": [
    "Pandas DataFrames are useful structures for working with data in Python. We can load our data from a CSV file directly into a DataFrame and display a sample of rows as output.\n",
    "\n",
    "The data we are using for this homework is from the \"Communities and Crime\" dataset available from UC Irvine's Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/communities+and+crime). It includes data about the different types of crimes among various communities, socioeconomic and racial data about each community, and information about the police force in each community.\n",
    "\n",
    "Pandas has a \"method\" called `read_csv` which lets us import the data we found on the web. We can then easily list this data by just calling the variable we assigned it to.\n",
    "\n",
    "The last column indicates whether or not there is a high rate of violent crime in the community (1 if yes, 0 if no). This is the target (Y) variable for the dataset. Running the two cells below will display a subset of the entries as well as a list of the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "error",
     "timestamp": 1599233556911,
     "user": {
      "displayName": "Andrew Thach",
      "photoUrl": "",
      "userId": "07507917617390649694"
     },
     "user_tz": 420
    },
    "id": "JMW4eiP-vhK_",
    "outputId": "10cdc80c-d779-472a-8d28-7f1e8ec9522f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.00           0.16          0.12          0.74          0.45   \n",
       "2        0.00           0.42          0.49          0.56          0.17   \n",
       "3        0.04           0.77          1.00          0.08          0.12   \n",
       "4        0.01           0.55          0.02          0.95          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32  ...   \n",
       "1         0.07         0.26         0.59         0.35        0.27  ...   \n",
       "2         0.04         0.39         0.47         0.28        0.32  ...   \n",
       "3         0.10         0.51         0.50         0.34        0.21  ...   \n",
       "4         0.05         0.38         0.38         0.23        0.36  ...   \n",
       "\n",
       "   LandArea  PopDens  PctUsePubTrans  PolicCars  PolicOperBudg  \\\n",
       "0      0.12     0.26            0.20       0.06           0.04   \n",
       "1      0.02     0.12            0.45       0.00           0.00   \n",
       "2      0.01     0.21            0.02       0.00           0.00   \n",
       "3      0.02     0.39            0.28       0.00           0.00   \n",
       "4      0.04     0.09            0.02       0.00           0.00   \n",
       "\n",
       "   LemasPctPolicOnPatr  LemasGangUnitDeploy  LemasPctOfficDrugUn  \\\n",
       "0                  0.9                  0.5                 0.32   \n",
       "1                  0.0                  0.0                 0.00   \n",
       "2                  0.0                  0.0                 0.00   \n",
       "3                  0.0                  0.0                 0.00   \n",
       "4                  0.0                  0.0                 0.00   \n",
       "\n",
       "   PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0             0.14                  0.0  \n",
       "1             0.00                  1.0  \n",
       "2             0.00                  1.0  \n",
       "3             0.00                  0.0  \n",
       "4             0.00                  0.0  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"communities.csv\")\n",
    "\n",
    "dataframe.head(5) # Show the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XvJgzRqvhLD",
    "outputId": "fdb0a6a4-76c3-46a8-ce7e-b2691632e869",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['population',\n",
       " 'householdsize',\n",
       " 'racepctblack',\n",
       " 'racePctWhite',\n",
       " 'racePctAsian',\n",
       " 'racePctHisp',\n",
       " 'agePct12t21',\n",
       " 'agePct12t29',\n",
       " 'agePct16t24',\n",
       " 'agePct65up',\n",
       " 'numbUrban',\n",
       " 'pctUrban',\n",
       " 'medIncome',\n",
       " 'pctWWage',\n",
       " 'pctWFarmSelf',\n",
       " 'pctWInvInc',\n",
       " 'pctWSocSec',\n",
       " 'pctWPubAsst',\n",
       " 'pctWRetire',\n",
       " 'medFamInc',\n",
       " 'perCapInc',\n",
       " 'whitePerCap',\n",
       " 'blackPerCap',\n",
       " 'indianPerCap',\n",
       " 'AsianPerCap',\n",
       " 'OtherPerCap',\n",
       " 'HispPerCap',\n",
       " 'NumUnderPov',\n",
       " 'PctPopUnderPov',\n",
       " 'PctLess9thGrade',\n",
       " 'PctNotHSGrad',\n",
       " 'PctBSorMore',\n",
       " 'PctUnemployed',\n",
       " 'PctEmploy',\n",
       " 'PctEmplManu',\n",
       " 'PctEmplProfServ',\n",
       " 'PctOccupManu',\n",
       " 'PctOccupMgmtProf',\n",
       " 'MalePctDivorce',\n",
       " 'MalePctNevMarr',\n",
       " 'FemalePctDiv',\n",
       " 'TotalPctDiv',\n",
       " 'PersPerFam',\n",
       " 'PctFam2Par',\n",
       " 'PctKids2Par',\n",
       " 'PctYoungKids2Par',\n",
       " 'PctTeen2Par',\n",
       " 'PctWorkMomYoungKids',\n",
       " 'PctWorkMom',\n",
       " 'NumIlleg',\n",
       " 'PctIlleg',\n",
       " 'NumImmig',\n",
       " 'PctImmigRecent',\n",
       " 'PctImmigRec5',\n",
       " 'PctImmigRec8',\n",
       " 'PctImmigRec10',\n",
       " 'PctRecentImmig',\n",
       " 'PctRecImmig5',\n",
       " 'PctRecImmig8',\n",
       " 'PctRecImmig10',\n",
       " 'PctSpeakEnglOnly',\n",
       " 'PctNotSpeakEnglWell',\n",
       " 'PctLargHouseFam',\n",
       " 'PctLargHouseOccup',\n",
       " 'PersPerOccupHous',\n",
       " 'PersPerOwnOccHous',\n",
       " 'PersPerRentOccHous',\n",
       " 'PctPersOwnOccup',\n",
       " 'PctPersDenseHous',\n",
       " 'PctHousLess3BR',\n",
       " 'MedNumBR',\n",
       " 'HousVacant',\n",
       " 'PctHousOccup',\n",
       " 'PctHousOwnOcc',\n",
       " 'PctVacantBoarded',\n",
       " 'PctVacMore6Mos',\n",
       " 'MedYrHousBuilt',\n",
       " 'PctHousNoPhone',\n",
       " 'PctWOFullPlumb',\n",
       " 'OwnOccLowQuart',\n",
       " 'OwnOccMedVal',\n",
       " 'OwnOccHiQuart',\n",
       " 'RentLowQ',\n",
       " 'RentMedian',\n",
       " 'RentHighQ',\n",
       " 'MedRent',\n",
       " 'MedRentPctHousInc',\n",
       " 'MedOwnCostPctInc',\n",
       " 'MedOwnCostPctIncNoMtg',\n",
       " 'NumInShelters',\n",
       " 'NumStreet',\n",
       " 'PctForeignBorn',\n",
       " 'PctBornSameState',\n",
       " 'PctSameHouse85',\n",
       " 'PctSameCity85',\n",
       " 'PctSameState85',\n",
       " 'LemasSwornFT',\n",
       " 'LemasSwFTPerPop',\n",
       " 'LemasSwFTFieldOps',\n",
       " 'LemasSwFTFieldPerPop',\n",
       " 'LemasTotalReq',\n",
       " 'LemasTotReqPerPop',\n",
       " 'PolicReqPerOffic',\n",
       " 'PolicPerPop',\n",
       " 'RacialMatchCommPol',\n",
       " 'PctPolicWhite',\n",
       " 'PctPolicBlack',\n",
       " 'PctPolicHisp',\n",
       " 'PctPolicAsian',\n",
       " 'PctPolicMinor',\n",
       " 'OfficAssgnDrugUnits',\n",
       " 'NumKindsDrugsSeiz',\n",
       " 'PolicAveOTWorked',\n",
       " 'LandArea',\n",
       " 'PopDens',\n",
       " 'PctUsePubTrans',\n",
       " 'PolicCars',\n",
       " 'PolicOperBudg',\n",
       " 'LemasPctPolicOnPatr',\n",
       " 'LemasGangUnitDeploy',\n",
       " 'LemasPctOfficDrugUn',\n",
       " 'PolicBudgPerPop',\n",
       " 'ViolentCrimesPerPop']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTcI6RA3vhLG"
   },
   "source": [
    "We can also extract the numerical values from a DataFrame into a Numpy array. Depending on the situation, these formats have various strengths and weaknesses. Numpy arrays are lightweight and behave much like a standard list, but do not support heterogenous data or many of the Pandas features for indexing and querying.\n",
    "\n",
    "Below, we extract the values from our dataframe, display the dimensions of the array, and display a subset of the rows and columns. You should find that the values displayed match with the above dataframe output.\n",
    "\n",
    "More information about array indexing in Numpy is available here: https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dc71TksbvhLG"
   },
   "outputs": [],
   "source": [
    "data = dataframe.values[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zErawG02vhLJ",
    "outputId": "3e20a585-08e6-44f2-f43e-eed1b74140db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 123)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErSsV6QWvhLN",
    "outputId": "656236d0-0477-4fc6-c4dd-b5aea514a0c1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19, 0.33, 0.02, 0.9 , 0.12],\n",
       "       [0.  , 0.16, 0.12, 0.74, 0.45],\n",
       "       [0.  , 0.42, 0.49, 0.56, 0.17],\n",
       "       [0.04, 0.77, 1.  , 0.08, 0.12],\n",
       "       [0.01, 0.55, 0.02, 0.95, 0.09]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lRdwhZXvhLP"
   },
   "source": [
    "### Problem 1.1: Creating X and Y (5 Points)\n",
    "\n",
    "Create arrays titled \"X\" and \"Y\", where X consists of all but the last column of data (for all rows) and Y is exclusively the last column. Print the shape of each array as accessed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5GFkffsvhLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 122)\n",
      "(1994,)\n"
     ]
    }
   ],
   "source": [
    "x = data[:,:-1].copy()\n",
    "y = data[:,-1].copy()\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVRA0kwnvhLS"
   },
   "source": [
    "### Problem 1.2: Creating Train and Test Sets (5 points)\n",
    "\n",
    "Create arrays titled: \n",
    "- X_train (first 1000 rows of X)\n",
    "- X_test (remaining rows of X)\n",
    "- Y_train (first 1000 rows of y)\n",
    "- Y_test (remaining rows of y)\n",
    "\n",
    "As the order of the records in the dataset are randomized, it is fine to simply use the beginning of the file as training and the rest as test.\n",
    "\n",
    "For the y arrays, you may want to consider using the <code>.ravel()</code> method for \"flattening\" the array from 2 dimensions down to 1. Print the shape of each array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEV4mIcpvhLS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 122)\n",
      "(1000,)\n",
      "(994, 122)\n",
      "(994,)\n"
     ]
    }
   ],
   "source": [
    "X_train = x[:1000,:].copy()\n",
    "X_test = x[1000:,:].copy()\n",
    "y_train = y[:1000].copy().ravel()\n",
    "y_test = y[1000:].copy().ravel()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFMuZppWvhLU"
   },
   "source": [
    "## Part 2: Training a Model\n",
    "\n",
    "Scikit-learn features modules for a wide variety of machine learning algorithms, such as logistic regression and decision trees. Read the documentation to understand how to train these models and generate predictions.\n",
    "- Logistic Regression documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Decision Tree documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kc5OzS4yvhLV"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, tree\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=5)\n",
    "logreg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lgYV52uvhLW"
   },
   "source": [
    "### Problem 2.1: Making Predictions with Decision Trees and Linear Regression (10 points)\n",
    "\n",
    "Using your X_train and Y_train arrays, train:\n",
    "- a Decision Tree model\n",
    "- a linear model via Logistic Regression (this may throw a DataConversionWarning which you can ignore)\n",
    "\n",
    "Using X_test, generate \"Y_hat\" predictions (one set of predictions with each model). Print the shape of each prediction array. For those new to scikit-learn, see the \"predict\" method within each model's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIFyO8zhvhLX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994,)\n",
      "(994,)\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train,y_train)\n",
    "y_hat_logreg=logreg.predict(X_test)\n",
    "print(y_hat_logreg.shape)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_hat_dt = dt.predict(X_test)\n",
    "print(y_hat_dt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSV7FUeevhLa"
   },
   "source": [
    "## Part 3: Evaluating a Model\n",
    "### Problem 3.1: Compute Prediction Error (5 points)\n",
    "Write a function which takes in 2 binary arrays as arguments (i.e. y and y_hat) and computes the prediction error as a decimal between 0 and 1. Use this function to compute the errors for your Decision Tree and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "we6p0orNvhLa"
   },
   "outputs": [],
   "source": [
    "def error(y, y_hat):\n",
    "    pred_err = np.sum(np.abs(y-y_hat))\n",
    "    return pred_err/y.size\n",
    "pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8_YAD3gvhLd",
    "outputId": "68810d5a-0564-438d-d5d6-d458f1d7febf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16297786720321933, 0.15090543259557343)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(error(y_test, y_hat_dt), error(y_test, y_hat_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tqd00jXqvhLf"
   },
   "source": [
    "## Part 4: Visualizing Tradeoffs (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFHydVO6vhLf"
   },
   "source": [
    "### Matplotlib\n",
    "\n",
    "We will be using the snazzy library 'matplotlib' for visualizations. Below we demonstrate some of its features in a generic plot. Hopefully it will be helpful for this next section, where we ask you to plot some tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKYLdPCDvhLf",
    "outputId": "67cdb573-1785-4b01-8d5b-324c66b7d3d5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Sample 100 random values from [0,1]\n",
    "y1_example = np.array([random.random() for i in range(100)])\n",
    "y2_example = np.array([i *0.01 for i in range(100)])\n",
    "# Create an array with the indices\n",
    "x_example = np.array(range(len(y1_example)))\n",
    "\n",
    "# Create a plot with a caption, X and Y legends, etc\n",
    "x_label = 'X value'\n",
    "y_label = 'Y value'\n",
    "plt.title('Example Plot')\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "\n",
    "\n",
    "plt.scatter(x_example, y1_example, color='red', label='Points')\n",
    "plt.plot(x_example, y1_example, color='blue', label='Line 1')\n",
    "plt.plot(x_example, y2_example, color='green', label='Line 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89Iuyv30vhLi"
   },
   "source": [
    "### Problem 4.1: Sample Size vs Generalization Error (10 points)\n",
    "\n",
    "Write code which creates training sets of size $n \\in \\{10,20,...,990,1000\\}$ by taking the first $n$ rows of X_train and Y_train (give these different names from the original arrays). Train Decision Tree and Logistic Regression models with each of these training sets, generate out-of-sample predictions using X_test, and compute error using Y_train as above.\n",
    "\n",
    "Generate a matplotlib plot with \"Sample Size\" as the X-axis and \"Test Error\" as the Y-axis. Plot lines for both the Decision Tree and Logistic Regression results. Plot your lines in different colors and include a legend to specify which line belongs to which model class. \n",
    "\n",
    "__Disclaimer:__ The results you will see show that test error decreases quickly the number of samples in the dataset. Often, in practice, it takes tens of thousands of training samples to see a meaningful decrease in test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "315J4GHSvhLi"
   },
   "outputs": [],
   "source": [
    "X_sets = np.array([(i+1)*10 for i in range(100)])\n",
    "X_err_logreg = np.zeros(shape=(X_sets.size,1))\n",
    "X_err_dt = np.zeros(shape=(X_sets.size,1))\n",
    "\n",
    "logreg = linear_model.LogisticRegression(max_iter=200)\n",
    "\n",
    "for i in X_sets:\n",
    "    logreg.fit(X_train[:i,:],y_train[:i])\n",
    "    y_hat_logreg_i = logreg.predict(X_test)\n",
    "    \n",
    "    dt.fit(X_train[:i,:],y_train[:i])\n",
    "    y_hat_dt_i = dt.predict(X_test)\n",
    "    \n",
    "    X_err_logreg[int(i/10-1)] = error(y_test, y_hat_logreg_i)\n",
    "    X_err_dt[int(i/10-1)] = error(y_test, y_hat_dt_i)\n",
    "    \n",
    "plt.title('Sample Size vs Generalization Error')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Test Error')\n",
    "\n",
    "plt.plot(X_sets, X_err_logreg, color='blue', label='Logistic Regression')\n",
    "plt.plot(X_sets, X_err_dt, color='red', label='Decision Tree')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6H3pKlvvhLk"
   },
   "source": [
    "### Problem 4.2: Model Complexity vs Generalization Error (10 points)\n",
    "\n",
    "Vary the max depth of the decision tree from 1 to 15. Plot the resulting error when training a model with all 1000 rows of X_train and Y_train. You can adjust the max depth by reinstantiating the DecisionTreeClassifier module with a max_depth parameter:\n",
    "\n",
    "`dt = tree.DecisionTreeClassifier(max_depth=i)`\n",
    "\n",
    "Generate a plot with \"Max Depth\" as the X-axis and \"Test Error\" as the Y-axis. Plot the error when predicting labels for X_train as well as X_test for each value of the maximum tree depth. Plot your lines in different colors and include a legend to specify which line belongs to which model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_WVGgh1vhLk"
   },
   "outputs": [],
   "source": [
    "X_depths = np.array([(i+1) for i in range(14)])\n",
    "X_err_train = np.zeros(shape=(X_sets.size,1))\n",
    "X_err_test = np.zeros(shape=(X_sets.size,1))\n",
    "\n",
    "for i in X_sets:\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=i)\n",
    "    \n",
    "    dt.fit(X_train[:1000,:],y_train[:1000])\n",
    "    y_hat_dt_test = dt.predict(X_test)\n",
    "    y_hat_dt_train = dt.predict(X_train)\n",
    "    \n",
    "    X_err_train[i-1] = error(y_train, y_hat_dt_train)\n",
    "    X_err_test[i-1] = error(y_test, y_hat_dt_test)\n",
    "    \n",
    "plt.title('Model Complexity vs Generalization Error')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Test Error')\n",
    "\n",
    "plt.plot(X_depths, X_err_test, color='blue', label='X_test')\n",
    "plt.plot(X_depths, X_err_train, color='red', label='X_train')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPFIHnytvhLm"
   },
   "source": [
    "## Part 5: Observing Error Disparities (30 points)\n",
    "\n",
    "In this section, you will explore the disparities in error for different \"groups\" of the dataset. The error disparity between two test sets with errors $\\epsilon_1$ and $\\epsilon_2$ is $|\\epsilon_1 - \\epsilon_2|$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bA8INPzvhLm"
   },
   "source": [
    "### Problem 5.1: Splitting by Feature Values (10 points)\n",
    "\n",
    "Write a function which takes in X and Y arrays, a column number, and a threshold. The function should return arrays X0 and Y0 containing all rows where the value in the specified column falls strictly below the threshold, as well as arrays X1 and Y1 containing all rows where the the value in the specified column is above or equal to the threshold. \n",
    "\n",
    "\n",
    "Numpy supports indexing via an array of values, which allows you to extract a non-contiguous subset of rows from an array. You might find this helpful. More information is available here: https://docs.scipy.org/doc/numpy-1.10.0/user/basics.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szUcFwzvvhLn"
   },
   "outputs": [],
   "source": [
    "def split_on_feature(X_test, Y_test, column, thresh):\n",
    "    \n",
    "    #Combines X_test and Y_test to view complete rows\n",
    "    XY_test = np.append(X_test, y_test.reshape(994,1), axis=1)\n",
    "    \n",
    "    X0_test = XY_test[XY_test[:,column] < thresh]\n",
    "    X1_test = XY_test[XY_test[:,column] >= thresh]\n",
    "    \n",
    "    #Divides it again into X_test and Y_test\n",
    "    Y0_test = X0_test[:,-1].copy()\n",
    "    X0_test = X0_test[:,:-1].copy()\n",
    "    Y1_test = X1_test[:,-1].copy()\n",
    "    X1_test = X1_test[:,:-1].copy()\n",
    "    \n",
    "    return (X0_test, X1_test, Y0_test, Y1_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26qpV7ldvhLo"
   },
   "source": [
    "### Problem 5.2: Calculating All Discrepancies (10 points)\n",
    "\n",
    "Now, let's evaluate the error disparities for the model you previously trained in Section 2.1. If you used the same naming conventions for sections 3 and 4, the models may have been overwritten. If that's the case, make sure to rerun the code in Section 2.1. \n",
    "\n",
    "For each feature in the dataset, use the function from 5.1 to split on that column when the threshold is set to 0.5. Then compute the error disparity for the feature by calculating the error of predictions made on both X0 and X1. \n",
    "\n",
    "This cell should print out the columns _by name_ (using the list of names in the Pandas dataframe) along with their corresponding error discrepancies, and should print in descending order of error discrepancy. You should omit columns where either of the splits have fewer than 100 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jcL36tDvhLp"
   },
   "outputs": [],
   "source": [
    "y_hat = logreg.predict(X_test)\n",
    "err_preds = np.zeros([122,])\n",
    "df = pd.DataFrame({'Column':dataframe.columns[:-1],\n",
    "                   'Disparity':err_preds})\n",
    "for i in range(0, X_test.shape[1]):\n",
    "    dispar_tuple = split_on_feature(X_test, y_test, i, 0.5)\n",
    "    \n",
    "    y_hat_X0 = logreg.predict(dispar_tuple[0])\n",
    "    y_hat_X1 = logreg.predict(dispar_tuple[1])\n",
    "    \n",
    "    if (y_hat_X0.size>100 and y_hat_X1.size>=100):\n",
    "        df.at[i,'Disparity'] = ( abs( error(y_hat_X0,dispar_tuple[2]) - error(y_hat_X1,dispar_tuple[3])) )\n",
    "        \n",
    "    #Intentionally leaving the columns where either of the splits have fewer than 100 rows as NaN\n",
    "    else: df.at[i,'Disparity'] = np.nan \n",
    "    \n",
    "df = df.sort_values(by='Disparity',ascending=False)\n",
    "\n",
    "df.set_index('Column').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGwU5_u8vhLq"
   },
   "source": [
    "### Problem 5.3: Other Types of Discrepancies (10 points)\n",
    "\n",
    "Instead of error disparities, let's compute two other types of errors that are of interest to us: False Negative Disparity and False Positive Disparity. \n",
    "\n",
    "For the feature racePctblack (percentage of population that is African-American), which is in column 2, compute the False Positive rate and False Negative rate using the provided functions. You should threshold the feature at 0.5 as earlier to create the two sets of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBX7lphUvhLr"
   },
   "outputs": [],
   "source": [
    "## INPUTS:\n",
    "# y - true labels\n",
    "# y_hat - predicted labels\n",
    "def fp_error(y, y_hat):\n",
    "    fp_errors = [np.maximum(y_hat[i] - y[i], 0) for i in range(len(y))]\n",
    "    return np.mean(fp_errors)\n",
    "\n",
    "## INPUTS:\n",
    "# y - true labels\n",
    "# y_hat - predicted labels\n",
    "def fn_error(y, y_hat):\n",
    "    fn_errors = [np.maximum(y[i] - y_hat[i], 0) for i in range(len(y))]\n",
    "    return np.mean(fn_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixm5g3b3vhLs",
    "outputId": "63ae1900-ad14-4387-d8f2-cf5cb7310190"
   },
   "outputs": [],
   "source": [
    "dis_tuple = split_on_feature(X_test, y_test, 2, 0.5)\n",
    "\n",
    "y_hat_X0 = logreg.predict(dispar_tuple[0])\n",
    "y_hat_X1 = logreg.predict(dispar_tuple[1])\n",
    "\n",
    "y0_fperr = fp_error(dispar_tuple[2],y_hat_X0)\n",
    "y0_fnerr = fn_error(dispar_tuple[2],y_hat_X0)\n",
    "y1_fperr = fp_error(dispar_tuple[3],y_hat_X1)\n",
    "y1_fnerr = fn_error(dispar_tuple[3],y_hat_X1)\n",
    "\n",
    "print('False Positive Error Rate of Communities with Above Median Black Population: ', y1_fperr)\n",
    "print('False Positive Error Rate of Communities with Below Median Black Population: ', y0_fperr)\n",
    "\n",
    "print('False Negative Error Rate of Communities with Above Median Black Population: ', y1_fnerr)\n",
    "print('False Negative Error Rate of Communities with Below Median Black Population: ', y0_fnerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TyI1c1uvhLu"
   },
   "source": [
    "## Part 6: Short Response Questions (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1DZt_wYvhLu"
   },
   "source": [
    "#### Q1: When training a machine learning model with some dataset, what are some assumptions we are making about the data? What are some things that it is important for us not to assume? Please give a few examples for each. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZ9yUr-UvhLv"
   },
   "source": [
    "In this case, we are assuming that there is somewhat of a relationship between our features; we consider the y results to be dependent on some or all of X features. \n",
    "\n",
    "In the other hand, we are making some decisive assumptions that could give us negative results when not taken into account. We are assuming that the error rate is somewhat uniform across all values. Finally, we are assuming that there isn't a high or complete correlation within values, because it wouldn't make our regression algorithm work. We could run tests to review the correctness of these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pE41hbB5vhLv"
   },
   "source": [
    "#### Q2: Why is it important to evaluate our model on data which was not used in training? What is the error rate on \"test\" or \"holdout\" data supposed to be a proxy for? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaN7hIuxvhLv"
   },
   "source": [
    "When we evaluate our model with different data that was used in the training, we are seeing if the algorithm was able to develop models that recognize patterns within the data, and didn't just recognize feature relationships within the training data. The eeror rate in the test data is a proxy for how much did the program work at making assumptions and models with the data. A high error rate in the test data means that the algorithm didn't learn much, but a low one means that it was succesful at reviewing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYyxHEH2vhLw"
   },
   "source": [
    "#### Q3: In your own words, explain the results of your plot from 4.1. Why does it make sense that these results occur? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoyRIOFPvhLw"
   },
   "source": [
    "The prediction error dicreases as more test values are given since the algorithm has more data to develop models that recognize patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERGdRbWYvhLw"
   },
   "source": [
    "#### Q4: In your own words, explain the results of your plot from 4.2. Why does it make sense that these results occur? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wlfciOxvhLx"
   },
   "source": [
    "When you have a high depth in your decision tree algoritm, it becomes really good at recognizing patterns within the train data. This is because it comes close to memorizing the data. When you give it test data, since the algorithm was overfitted, it is more difficult to make predictions based on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W31Qzr74vhLx"
   },
   "source": [
    "#### Q5: In your own words, explain the results of section 5.3. What are some possible implications of this model in terms of unfairness? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4YF242hvhLy"
   },
   "source": [
    "As we are dealing with race as a percentage of population, we could have a case where the results are dependent from the given values. Maybe in some way there is some cultural influence that affects these variables into seeming that there is a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UChV9F0vhLz"
   },
   "source": [
    "#### Q6: Look through the available features on the dataset we used, available at http://archive.ics.uci.edu/ml/datasets/communities+and+crime. What are two attributes that you would expect to have high error disparity? What are two attributes you would expect to have low error disparity? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nB4JM70ovhLz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-54DAPpwvhL0"
   },
   "source": [
    "## EXTRA CREDIT: Part 7 (5-10 points)\n",
    "\n",
    "Play around with the data and generate some kind of plot (via matplotlib) that you find interesting. Write a few sentences about your process, what you found, and what you think it suggests about the data. This could be an evaluation of multiple model classes, a statistical analysis of different features, unsupervised analysis, extending the investigation into error discrepancies, or anything else you can think of. \n",
    "\n",
    "Any well-justified solution will earn up to 5 points of extra credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJ4V6NqSvhL0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSE146_Lab1--[YOUR NAME HERE]--2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
